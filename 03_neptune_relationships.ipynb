{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89495087-278c-407c-a67c-a94ef64cdff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from s3://team4bucket-mmm/Patent_Data_Kaggle/Patent_Data.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7910/992024916.py:13: DtypeWarning: Columns (13,14,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(StringIO(content), sep=',')  # Using comma as separator for Patent_Data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating node files in relationships...\n",
      "Creating edge files in relationships...\n",
      "Uploading relationships/case_nodes.csv to s3://team4bucket-mmm/neptune_import/case_nodes.csv\n",
      "Uploading relationships/company_nodes.csv to s3://team4bucket-mmm/neptune_import/company_nodes.csv\n",
      "Uploading relationships/judge_nodes.csv to s3://team4bucket-mmm/neptune_import/judge_nodes.csv\n",
      "Uploading relationships/court_nodes.csv to s3://team4bucket-mmm/neptune_import/court_nodes.csv\n",
      "Uploading relationships/patent_nodes.csv to s3://team4bucket-mmm/neptune_import/patent_nodes.csv\n",
      "Uploading relationships/filed_lawsuit_edges.csv to s3://team4bucket-mmm/neptune_import/filed_lawsuit_edges.csv\n",
      "Uploading relationships/defendant_edges.csv to s3://team4bucket-mmm/neptune_import/defendant_edges.csv\n",
      "Uploading relationships/judge_edges.csv to s3://team4bucket-mmm/neptune_import/judge_edges.csv\n",
      "Uploading relationships/court_edges.csv to s3://team4bucket-mmm/neptune_import/court_edges.csv\n",
      "Uploading relationships/patent_edges.csv to s3://team4bucket-mmm/neptune_import/patent_edges.csv\n",
      "Uploaded results to s3://team4bucket-mmm/neptune_import/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Load the raw data from S3\n",
    "def load_data_from_s3(bucket_name, file_key):\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        data = pd.read_csv(StringIO(content), sep=',')  # Using comma as separator for Patent_Data.csv\n",
    "        # Clean column names (remove any trailing \\r)\n",
    "        data.columns = [col.strip('\\r') for col in data.columns]\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from S3: {e}\")\n",
    "        raise\n",
    "\n",
    "# Create node CSV files for Neptune\n",
    "def create_node_files(data, output_dir):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # 1. Case Nodes\n",
    "    case_nodes = data[['case_no', 'filed_date', 'filing_year', 'status', 'court', \n",
    "                        'patents', 'cause_of_action', 'closed_date']].copy()\n",
    "    case_nodes['~id'] = case_nodes['case_no']\n",
    "    case_nodes['~label'] = 'Case'\n",
    "    case_nodes.to_csv(f\"{output_dir}/case_nodes.csv\", index=False)\n",
    "    \n",
    "    # 2. Company Nodes (from both plaintiffs and defendants)\n",
    "    # Extract all plaintiff companies\n",
    "    plaintiff_companies = set()\n",
    "    for plaintiff_str in data['plaintiff'].dropna():\n",
    "        for plaintiff in plaintiff_str.split('|'):\n",
    "            plaintiff_companies.add(plaintiff.strip())\n",
    "            \n",
    "    # Extract all defendant companies\n",
    "    defendant_companies = set()\n",
    "    for defendant_str in data['defendant'].dropna():\n",
    "        for defendant in defendant_str.split('|'):\n",
    "            defendant_companies.add(defendant.strip())\n",
    "            \n",
    "    # Combine all companies and create nodes\n",
    "    all_companies = list(plaintiff_companies.union(defendant_companies))\n",
    "    company_nodes = pd.DataFrame({\n",
    "        '~id': all_companies,\n",
    "        '~label': 'Company'\n",
    "    })\n",
    "    company_nodes.to_csv(f\"{output_dir}/company_nodes.csv\", index=False)\n",
    "    \n",
    "    # 3. Judge Nodes\n",
    "    judge_nodes = pd.DataFrame({\n",
    "        '~id': data['judge'].dropna().unique(),\n",
    "        '~label': 'Judge'\n",
    "    })\n",
    "    judge_nodes.to_csv(f\"{output_dir}/judge_nodes.csv\", index=False)\n",
    "    \n",
    "    # 4. Court Nodes\n",
    "    court_nodes = pd.DataFrame({\n",
    "        '~id': data['court'].dropna().unique(),\n",
    "        '~label': 'Court'\n",
    "    })\n",
    "    court_nodes.to_csv(f\"{output_dir}/court_nodes.csv\", index=False)\n",
    "    \n",
    "    # 5. Patent Nodes (if patent numbers are available)\n",
    "    patent_ids = set()\n",
    "    for patent_str in data['patents'].dropna():\n",
    "        for patent in patent_str.split('|'):\n",
    "            if patent.strip():\n",
    "                patent_ids.add(patent.strip())\n",
    "                \n",
    "    if patent_ids:\n",
    "        patent_nodes = pd.DataFrame({\n",
    "            '~id': list(patent_ids),\n",
    "            '~label': 'Patent'\n",
    "        })\n",
    "        patent_nodes.to_csv(f\"{output_dir}/patent_nodes.csv\", index=False)\n",
    "\n",
    "# Create edge CSV files for Neptune\n",
    "def create_edge_files(data, output_dir):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # 1. FILED_LAWSUIT edges (plaintiff -> case)\n",
    "    filed_lawsuit_rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        case_id = row['case_no']\n",
    "        if pd.notna(row['plaintiff']):\n",
    "            for plaintiff in row['plaintiff'].split('|'):\n",
    "                plaintiff = plaintiff.strip()\n",
    "                filed_lawsuit_rows.append({\n",
    "                    '~from': plaintiff,\n",
    "                    '~to': case_id,\n",
    "                    '~label': 'FILED_LAWSUIT',\n",
    "                    'filed_date': row['filed_date']\n",
    "                })\n",
    "    \n",
    "    filed_lawsuit_df = pd.DataFrame(filed_lawsuit_rows)\n",
    "    if not filed_lawsuit_df.empty:\n",
    "        filed_lawsuit_df.to_csv(f\"{output_dir}/filed_lawsuit_edges.csv\", index=False)\n",
    "    \n",
    "    # 2. DEFENDANT_IN edges (defendant -> case)\n",
    "    defendant_rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        case_id = row['case_no']\n",
    "        if pd.notna(row['defendant']):\n",
    "            for defendant in row['defendant'].split('|'):\n",
    "                defendant = defendant.strip()\n",
    "                defendant_rows.append({\n",
    "                    '~from': defendant,\n",
    "                    '~to': case_id,\n",
    "                    '~label': 'DEFENDANT_IN'\n",
    "                })\n",
    "    \n",
    "    defendant_df = pd.DataFrame(defendant_rows)\n",
    "    if not defendant_df.empty:\n",
    "        defendant_df.to_csv(f\"{output_dir}/defendant_edges.csv\", index=False)\n",
    "    \n",
    "    # 3. PRESIDED_OVER edges (judge -> case)\n",
    "    judge_rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        case_id = row['case_no']\n",
    "        if pd.notna(row['judge']):\n",
    "            judge_rows.append({\n",
    "                '~from': row['judge'],\n",
    "                '~to': case_id,\n",
    "                '~label': 'PRESIDED_OVER'\n",
    "            })\n",
    "    \n",
    "    judge_df = pd.DataFrame(judge_rows)\n",
    "    if not judge_df.empty:\n",
    "        judge_df.to_csv(f\"{output_dir}/judge_edges.csv\", index=False)\n",
    "    \n",
    "    # 4. VENUE_FOR edges (court -> case)\n",
    "    court_rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        case_id = row['case_no']\n",
    "        if pd.notna(row['court']):\n",
    "            court_rows.append({\n",
    "                '~from': row['court'],\n",
    "                '~to': case_id,\n",
    "                '~label': 'VENUE_FOR'\n",
    "            })\n",
    "    \n",
    "    court_df = pd.DataFrame(court_rows)\n",
    "    if not court_df.empty:\n",
    "        court_df.to_csv(f\"{output_dir}/court_edges.csv\", index=False)\n",
    "    \n",
    "    # 5. INVOLVES_PATENT edges (case -> patent)\n",
    "    patent_rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        case_id = row['case_no']\n",
    "        if pd.notna(row['patents']):\n",
    "            for patent in row['patents'].split('|'):\n",
    "                patent = patent.strip()\n",
    "                if patent:\n",
    "                    patent_rows.append({\n",
    "                        '~from': case_id,\n",
    "                        '~to': patent,\n",
    "                        '~label': 'INVOLVES_PATENT'\n",
    "                    })\n",
    "    \n",
    "    patent_df = pd.DataFrame(patent_rows)\n",
    "    if not patent_df.empty:\n",
    "        patent_df.to_csv(f\"{output_dir}/patent_edges.csv\", index=False)\n",
    "\n",
    "# Main function to process the data from S3\n",
    "def process_patent_data_for_neptune_from_s3(s3_bucket, s3_key, output_dir, output_s3_bucket=None):\n",
    "    print(f\"Loading data from s3://{s3_bucket}/{s3_key}...\")\n",
    "    data = load_data_from_s3(s3_bucket, s3_key)\n",
    "    \n",
    "    print(f\"Creating node files in {output_dir}...\")\n",
    "    create_node_files(data, output_dir)\n",
    "    \n",
    "    print(f\"Creating edge files in {output_dir}...\")\n",
    "    create_edge_files(data, output_dir)\n",
    "    \n",
    "    # Optionally upload results back to S3\n",
    "    if output_s3_bucket:\n",
    "        upload_results_to_s3(output_dir, output_s3_bucket)\n",
    "        print(f\"Uploaded results to s3://{output_s3_bucket}/neptune_import/\")\n",
    "    else:\n",
    "        print(f\"Done! Files are ready for Neptune import in {output_dir}\")\n",
    "\n",
    "# Function to upload results back to S3\n",
    "def upload_results_to_s3(local_dir, s3_bucket, s3_prefix='neptune_import'):\n",
    "    s3_client = boto3.client('s3')\n",
    "    for filename in os.listdir(local_dir):\n",
    "        local_path = os.path.join(local_dir, filename)\n",
    "        s3_key = f\"{s3_prefix}/{filename}\"\n",
    "        print(f\"Uploading {local_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "        s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your actual S3 bucket and key\n",
    "    S3_BUCKET = \"team4bucket-mmm\"  # <-- INSERT YOUR BUCKET NAME HERE\n",
    "    S3_KEY = \"Patent_Data_Kaggle/Patent_Data.csv\"    # <-- Path to your Patent_Data.csv file\n",
    "    \n",
    "    # Local directory for output files\n",
    "    OUTPUT_DIR = \"relationships\"\n",
    "    \n",
    "    # Optional: S3 bucket for output files (if you want to upload results back to S3)\n",
    "    OUTPUT_S3_BUCKET = \"team4bucket-mmm\"  # <-- Optional, remove if not needed\n",
    "    \n",
    "    process_patent_data_for_neptune_from_s3(S3_BUCKET, S3_KEY, OUTPUT_DIR, OUTPUT_S3_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473a2e3-e468-4303-8c3d-5a7e2367ef46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
